{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T10:35:42.398508Z",
     "iopub.status.busy": "2025-07-03T10:35:42.394861Z",
     "iopub.status.idle": "2025-07-03T10:35:42.487007Z",
     "shell.execute_reply": "2025-07-03T10:35:42.465645Z",
     "shell.execute_reply.started": "2025-07-03T10:35:42.398400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1338044834.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_26/1338044834.py\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    def preprocess('text'):\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE,\n",
    "              output_dim=256,\n",
    "              input_length=MAX_LEN,\n",
    "              mask_zero=True),\n",
    "\n",
    "    Bidirectional(LSTM(128, return_sequences=True,\n",
    "                     kernel_regularizer=l2(0.01),\n",
    "                     recurrent_dropout=0.2)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Bidirectional(LSTM(64)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy',\n",
    "             tf.keras.metrics.Precision(name='precision'),\n",
    "             tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2f}\")\n",
    "print(f\"Test Precision: {test_precision:.2f}\")\n",
    "print(f\"Test Recall: {test_recall:.2f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=emotion_labels))\n",
    "\n",
    "\n",
    "# Sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "for idx in sample_indices:\n",
    "    text_sample = test_df.iloc[idx]['text']\n",
    "    true_emotion = emotion_labels[y_true[idx]]\n",
    "    pred_emotion = emotion_labels[y_pred_classes[idx]]\n",
    "\n",
    "\n",
    "    print(f\"\\nText: {texts[idx][:100]}...\")\n",
    "    print(f\"True: {emotion_labels[y_true[idx]]}\")\n",
    "    print(f\"Pred: {emotion_labels[y_pred_classes[idx]]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
